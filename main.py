import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from queue import Queue

# ==============================
# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ØµÙ„ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡
# ==============================
MAX_PAGES = 30          # Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ø§ØªÛŒ Ú©Ù‡ Ø®Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
REQUEST_TIMEOUT = 5      # ØªØ§ÛŒÙ…â€ŒØ§ÙˆØª Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ (Ø«Ø§Ù†ÛŒÙ‡)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (TriangleCrawler Research Bot)"
}


# ==============================
# 1) Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù„ÛŒÙ†Ú©
# Ø­Ø°Ù fragment Ùˆ Ø§Ø³Ù„Ø´ Ø§Ø¶Ø§ÙÙ‡
# ==============================
def normalize_url(url):
    parsed = urlparse(url)
    normalized = parsed.scheme + "://" + parsed.netloc + parsed.path
    return normalized.rstrip("/")


# ==============================
# 2) Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø§Ø² ØµÙØ­Ù‡
# ==============================
def extract_links(url):
    links = set()

    try:
        response = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)
        if "text/html" not in response.headers.get("Content-Type", ""):
            return links  # ÙÙ‚Ø· HTML Ù…Ù‡Ù…Ù‡

        soup = BeautifulSoup(response.text, "html.parser")

        for tag in soup.find_all("a", href=True):
            href = tag["href"]
            full_url = urljoin(url, href)
            parsed = urlparse(full_url)

            if parsed.scheme in ["http", "https"]:
                links.add(normalize_url(full_url))

    except Exception as e:
        print(f"Error fetching {url} -> {e}")

    return links


# ==============================
# 3) Ø®Ø²Ø´ ÙˆØ¨ Ùˆ Ø³Ø§Ø®Øª Ú¯Ø±Ø§Ù
# ==============================
def crawl_web(seeds):
    visited = set()
    graph = {}
    queue = Queue()

    for seed in seeds:
        queue.put(normalize_url(seed))

    while not queue.empty() and len(visited) < MAX_PAGES:
        current_url = queue.get()

        if current_url in visited:
            continue

        print(f"Crawling: {current_url}")
        visited.add(current_url)

        links = extract_links(current_url)
        graph[current_url] = links

        for link in links:
            if link not in visited:
                queue.put(link)

    return graph


# ==============================
# 4) Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø«Ù„Ø«â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø¬Ø§Ø¹ÛŒ
# ==============================
def find_triangles(graph):
    triangles = set()

    for a in graph:
        for b in graph[a]:            # a â†’ b
            if b not in graph or a == b:
                continue

            for c in graph[b]:        # b â†’ c
                if c not in graph or b == c or a == c:
                    continue

                if a in graph[c]:     # c â†’ a
                    tri = tuple(sorted([a, b, c]))
                    triangles.add(tri)

    return triangles


# ==============================
# 5) Ú†Ø§Ù¾ Ú¯Ø±Ø§Ù (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
# ==============================
def print_graph(graph):
    print("\n====== LINK GRAPH ======")
    for page, links in graph.items():
        print(f"{page} -> {len(links)} links")


# ==============================
# 6) Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡
# ==============================
if __name__ == "__main__":
    seeds = [
        "https://quera.org/dashboard"
    ]

    print("\nğŸš€ Starting Crawl...\n")
    graph = crawl_web(seeds)

    print_graph(graph)

    print("\nğŸ”º Searching for Triangular References...\n")
    triangles = find_triangles(graph)

    if not triangles:
        print("No triangular link patterns found.")
    else:
        print("Found Triangles:")
        for t in triangles:
            print("A â†’ B â†’ C â†’ A :", t)
